*Related Work*
- The DiT: Diffusion Transformer - by Meta (company that own facebook)
- The Sana model: LinearDiT - by Nvidia
- The DiffiT: Vision Transformer Transformer - by Nvidia
- Attension is all you need -> this for the sinusoidal embedding(positional embedding for timesteps)

*GOAL*
- The goal for this project is to:
+ First, understanding the way the diffusion model work
+ Secondly, apply the TMSA(Time-dependance Multihead) from DiffiT and the LinearDiT from Sana model or from other way to archive
that to build a guidance text-to-image generation model.
+ Thirdly, train the model on Cifar-100 -> imagenet256 for testing
+ Last, train the model on custom danbooru dataset for production model.

*Roadmap*
+ Building the embbeder for timesteps and text-prompt. -> DONE
+ Building the Linear Attention block for model 

*To-do*
+ Build simple Multihead and single-head attention block for testing.
                    |
+ Build the Linear Attention block
                    |
+ Build the TMSA block 
                    |
+ Build the linear TMSA block